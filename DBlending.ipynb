{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blend_table(conn_old, conn_new, table_name,\n",
    "                key_cols=('Country','FIPS_CODE'),\n",
    "                meta_cols=('Earliest','MostRecent')):\n",
    "    \"\"\"\n",
    "    Blend table_name from conn_old into conn_new:\n",
    "      - Detect all numeric 'year' columns in both tables\n",
    "      - Build full year-range (min → max) and include any non-year data cols\n",
    "      - Reindex both tables so they share exactly those columns (gap-years become NaN)\n",
    "      - Wherever new is NaN, backfill from old\n",
    "      - Recompute Earliest & MostRecent from the full year set\n",
    "      - Drop & recreate table in conn_new\n",
    "    \"\"\"\n",
    "    # ─── 1. Load old & new tables ──────────────────────────────────────────────\n",
    "    df_old = pd.read_sql_query(f\"SELECT * FROM [{table_name}]\", conn_old)\n",
    "    df_new = pd.read_sql_query(f\"SELECT * FROM [{table_name}]\", conn_new)\n",
    "\n",
    "    # ─── 2. Drop any existing meta columns in df_new so they get rebuilt later ─\n",
    "    df_new = df_new.drop(columns=[c for c in meta_cols if c in df_new], errors='ignore')\n",
    "\n",
    "    # ─── 3. Identify candidate data columns (everything except keys + meta) ───\n",
    "    cols_old = set(df_old.columns) - set(key_cols) - set(meta_cols)\n",
    "    cols_new = set(df_new.columns) - set(key_cols) - set(meta_cols)\n",
    "\n",
    "    # ─── 4. Split year-columns vs. other data-columns ─────────────────────────\n",
    "    #    year-cols are those whose names are all digits\n",
    "    year_cols_old = [c for c in cols_old if c.isdigit()]\n",
    "    year_cols_new = [c for c in cols_new if c.isdigit()]\n",
    "    all_year_nums = [int(y) for y in year_cols_old + year_cols_new]\n",
    "\n",
    "    if all_year_nums:\n",
    "        year_min, year_max = min(all_year_nums), max(all_year_nums)\n",
    "        full_year_cols = [str(y) for y in range(year_min, year_max+1)]\n",
    "    else:\n",
    "        full_year_cols = []\n",
    "\n",
    "    other_data_cols = sorted((cols_old | cols_new) - set(full_year_cols))\n",
    "\n",
    "    all_data_cols = other_data_cols + full_year_cols\n",
    "\n",
    "    # ─── 5. Reindex both DFs to have exactly key_cols + all_data_cols ──────────\n",
    "    df_old = df_old.reindex(columns=list(key_cols) + all_data_cols)\n",
    "    df_new = df_new.reindex(columns=list(key_cols) + all_data_cols)\n",
    "\n",
    "    # ─── 6. Align on the key index & combine (new overrides, then old fills) ───\n",
    "    df_old_i = df_old.set_index(list(key_cols))\n",
    "    df_new_i = df_new.set_index(list(key_cols))\n",
    "    df_blend_i = df_new_i.combine_first(df_old_i)\n",
    "\n",
    "    # ─── 7. Back to a flat DataFrame ─────────────────────────────────────────\n",
    "    df_blend = df_blend_i.reset_index()\n",
    "\n",
    "    # ─── 8. Ensure numeric dtype on the year columns ──────────────────────────\n",
    "    for c in full_year_cols:\n",
    "        df_blend[c] = pd.to_numeric(df_blend[c], errors='coerce')\n",
    "\n",
    "    # ─── 9. Recompute Earliest & MostRecent from only the year sequence ───────\n",
    "    if full_year_cols:\n",
    "        df_blend['Earliest'] = (\n",
    "            df_blend[full_year_cols]\n",
    "            .apply(lambda row: row.dropna().iloc[0] if row.dropna().size else np.nan, axis=1)\n",
    "        )\n",
    "        df_blend['MostRecent'] = (\n",
    "            df_blend[full_year_cols]\n",
    "            .apply(lambda row: row.dropna().iloc[-1] if row.dropna().size else np.nan, axis=1)\n",
    "        )\n",
    "    else:\n",
    "        df_blend['Earliest'] = np.nan\n",
    "        df_blend['MostRecent'] = np.nan\n",
    "\n",
    "    # ─── 10. Write back: drop + recreate + append ──────────────────────────────\n",
    "    cur = conn_new.cursor()\n",
    "    cur.execute(f\"DROP TABLE IF EXISTS [{table_name}];\")\n",
    "\n",
    "    # Build CREATE TABLE DDL\n",
    "    ddl_cols = (\n",
    "        \", \".join(f\"[{col}] DOUBLE\" for col in all_data_cols)\n",
    "        + \", [Earliest] DOUBLE, [MostRecent] DOUBLE\"\n",
    "    )\n",
    "    create_sql = (\n",
    "        f\"CREATE TABLE [{table_name}] \"\n",
    "        f\"(Country VARCHAR(255), FIPS_CODE VARCHAR(255), {ddl_cols});\"\n",
    "    )\n",
    "    cur.execute(create_sql)\n",
    "\n",
    "    # Bulk-insert\n",
    "    df_blend.to_sql(table_name, conn_new, if_exists='append', index=False)\n",
    "    conn_new.commit()\n",
    "\n",
    "\n",
    "def needs_blending(conn_old, conn_new, table_name,\n",
    "                   key_cols=('Country','FIPS_CODE')):\n",
    "    \"\"\"\n",
    "    Return True if:\n",
    "      • there's any year-col in old NOT in new, or\n",
    "      • for any shared year-col, new has NaN but old has a value.\n",
    "    \"\"\"\n",
    "    # Load both tables\n",
    "    df_old = pd.read_sql_query(f\"SELECT * FROM [{table_name}];\", conn_old)\n",
    "    df_new = pd.read_sql_query(f\"SELECT * FROM [{table_name}];\", conn_new)\n",
    "\n",
    "    # Identify year-columns (names all digits)\n",
    "    years_old = {c for c in df_old.columns if c.isdigit()}\n",
    "    years_new = {c for c in df_new.columns if c.isdigit()}\n",
    "\n",
    "    # 1) Old-only year columns → need to blend\n",
    "    if years_old - years_new:\n",
    "        return True\n",
    "\n",
    "    # 2) Check overlapping years for fillable gaps\n",
    "    common_years = sorted(years_old & years_new)\n",
    "    if not common_years:\n",
    "        return False\n",
    "\n",
    "    # Align on key and build mask in one go\n",
    "    df_old_i = df_old.set_index(list(key_cols))\n",
    "    df_new_i = df_new.set_index(list(key_cols))\n",
    "    mask = df_new_i[common_years].isna() & df_old_i[common_years].notna()\n",
    "\n",
    "    return mask.values.any()\n",
    "\n",
    "\n",
    "def blend_databases(path_old, path_new, tables=None):\n",
    "    conn_old = sqlite3.connect(path_old)\n",
    "    conn_new = sqlite3.connect(path_new)\n",
    "\n",
    "    if tables is None:\n",
    "        dd = pd.read_sql_query(\"SELECT [Table] FROM DataDict;\", conn_new)\n",
    "        tables = dd['Table'].tolist()\n",
    "\n",
    "    for tbl in tables:\n",
    "        if not needs_blending(conn_old, conn_new, tbl):\n",
    "            print(f\"↩️  Skipping {tbl}: no missing columns or fillable gaps.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            blend_table(conn_old, conn_new, tbl)  # your full blend_table()\n",
    "            print(f\"✔ Blended {tbl}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✖ Failed {tbl}: {e}\")\n",
    "\n",
    "    conn_old.close()\n",
    "    conn_new.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_db = r\"YOUR_PATH/IFsHistSeries.db\" # full path to the old IFsHistSeries.db\n",
    "new_db = r\"YOUR_PATH/IFsDataImport.db\" # path to the IFsDataImport.db\n",
    "blend_databases(old_db, new_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
